{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9450363c",
   "metadata": {},
   "source": [
    "Constructing LeanDojo Benchmark (Lean 3)\n",
    "===================================\n",
    "\n",
    "This script uses [LeanDojo](https://leandojo.org/) to construct LeanDojo Benchmark used in our paper:\n",
    "\n",
    "[LeanDojo: Theorem Proving with Retrieval-Augmented Language Models](https://leandojo.org/)      \n",
    "NeurIPS 2023 (Datasets and Benchmarks Track)  \n",
    "[Kaiyu Yang](https://yangky11.github.io/), [Aidan Swope](https://aidanswope.com/about), [Alex Gu](https://minimario.github.io/), [Rahul Chalamala](https://rchalamala.github.io/), [Peiyang Song](https://peiyang-song.github.io/), [Shixing Yu](https://billysx.github.io/), [Saad Godil](https://www.linkedin.com/in/saad-godil-9728353/), [Ryan Prenger](https://www.linkedin.com/in/ryan-prenger-18797ba1/), [Anima Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/)\n",
    "\n",
    "The dataset is constructed from [mathlib](https://github.com/leanprover-community/mathlib/tree/19c869efa56bbb8b500f2724c0b77261edbfa28c) (`19c869efa56bbb8b500f2724c0b77261edbfa28c`) and will be saved to `../leandojo_benchmark`. It includes 2000 theorems for validation, 2000 theorems for testing, and the rest for training. Please refer to our paper for details. For most use cases, you shouldn't need to generate the data and can directly use our official LeanDojo Benchmark downloadable [here](https://doi.org/10.5281/zenodo.8016385).\n",
    "\n",
    "This script is for Lean 3. We also have a [version for Lean 4](https://github.com/lean-dojo/LeanDojo/blob/main/scripts/generate-benchmark-lean4.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda71d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from ray.util.actor_pool import ActorPool\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import lean_dojo\n",
    "from lean_dojo import *\n",
    "from lean_dojo.constants import LEAN3_DEPS_DIR\n",
    "\n",
    "random.seed(3407)  # https://arxiv.org/abs/2109.08203\n",
    "\n",
    "URL = \"https://github.com/leanprover-community/mathlib\"\n",
    "COMMIT = \"19c869efa56bbb8b500f2724c0b77261edbfa28c\"\n",
    "DST_DIR = Path(\"../leandojo_benchmark\")\n",
    "NUM_VAL = NUM_TEST = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043faf8",
   "metadata": {},
   "source": [
    "## Splitting the Theorems\n",
    "\n",
    "We will split the theorems into train/val/test using two different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdb4e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_NAME = str  # train/val/test\n",
    "SPLIT = Dict[SPLIT_NAME, List[TracedTheorem]]\n",
    "SPLIT_STRATEGY = str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501cac9",
   "metadata": {},
   "source": [
    "### Splitting Randomly\n",
    "\n",
    "The first and the simplest strategy is splitting the theorems randomly, which can be implemented by a random shuffle followed by a sequential split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7bcf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_sequentially(\n",
    "    traced_theorems: List[TracedTheorem],\n",
    ") -> SPLIT:\n",
    "    \"\"\"Split ``traced_theorems`` sequentially into train/val/test.\"\"\"\n",
    "    num_theorems = len(traced_theorems)\n",
    "    num_train = num_theorems - NUM_VAL - NUM_TEST\n",
    "    return {\n",
    "        \"train\": traced_theorems[:num_train],\n",
    "        \"val\": traced_theorems[num_train : num_train + NUM_VAL],\n",
    "        \"test\": traced_theorems[num_train + NUM_VAL :],\n",
    "    }\n",
    "\n",
    "\n",
    "def split_randomly(\n",
    "    traced_theorems: List[TracedTheorem],\n",
    ") -> SPLIT:\n",
    "    \"\"\"Split ``traced_theorems`` randomly into train/val/test.\"\"\"\n",
    "    logger.info(\"Splitting the theorems randomly\")\n",
    "    traced_theorems = copy(traced_theorems)\n",
    "    random.shuffle(traced_theorems)\n",
    "    return _split_sequentially(traced_theorems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e6399",
   "metadata": {},
   "source": [
    "### Splitting by Premise\n",
    "\n",
    "The second strategy is splitting by premise. We want to test the prover's capability in using novel premises, i.e., premises that have never been used in training. Please see the implementation below. Note that validation and testing theorems may share premises. So the **testing performance should be reported using models trained on the training set only, NOT training plus validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e1fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_premise(\n",
    "    traced_theorems: List[TracedTheorem],\n",
    ") -> SPLIT:\n",
    "    \"\"\"\n",
    "    Split theorems into train/val/test so that proofs in val/test rely on at\n",
    "    least one novel premise that does not appear in train.\n",
    "    \"\"\"\n",
    "    logger.info(\"Splitting the theorems by premises\")\n",
    "\n",
    "    # Figure out the number of theorems in train/val/test.\n",
    "    num_theorems = len(traced_theorems)\n",
    "    num_val_test = NUM_VAL + NUM_TEST\n",
    "    num_train = num_theorems - num_val_test\n",
    "    theorems_val_test = set()\n",
    "\n",
    "    # Map each premise to a list of theorems using it.\n",
    "    theorems_by_premises = defaultdict(list)\n",
    "    for t in traced_theorems:\n",
    "        for p in t.get_premise_full_names():\n",
    "            theorems_by_premises[p].append(t)\n",
    "\n",
    "    # Sort the premises by the number of theorems using them (in ascending order).\n",
    "    theorems_by_premises = sorted(theorems_by_premises.items(), key=lambda x: len(x[1]))\n",
    "\n",
    "    # For each premise, put all theorems using it into val_test so that it does not appear in train.\n",
    "    for _, thms in theorems_by_premises:\n",
    "        if len(theorems_val_test) < num_val_test:\n",
    "            theorems_val_test.update(thms)\n",
    "\n",
    "    # All other theorems go to train.\n",
    "    theorems_train = [t for t in traced_theorems if t not in theorems_val_test]\n",
    "    theorems_val_test = list(theorems_val_test)\n",
    "    random.shuffle(theorems_val_test)\n",
    "\n",
    "    return {\n",
    "        \"train\": theorems_train,\n",
    "        \"val\": theorems_val_test[:NUM_VAL],\n",
    "        \"test\": theorems_val_test[NUM_VAL:],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89c84e",
   "metadata": {},
   "source": [
    "Given a traced repo, we can split the theorems using these strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8509a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(traced_repo: TracedRepo) -> Dict[SPLIT_STRATEGY, SPLIT]:\n",
    "    traced_theorems = traced_repo.get_traced_theorems()\n",
    "    logger.info(f\"{len(traced_theorems)} theorems in total\")\n",
    "\n",
    "    return {\n",
    "        \"random\": split_randomly(traced_theorems),\n",
    "        \"novel_premises\": split_by_premise(traced_theorems),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bffa9",
   "metadata": {},
   "source": [
    "## Exporting the Data\n",
    "\n",
    "Once theorems are split into train/val/test. We export them to JSON formats that can be easily used in machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac984a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_file_path(traced_repo: TracedRepo, thm: TracedTheorem) -> str:\n",
    "    if thm.repo == traced_repo.repo:\n",
    "        # The theorem belongs to the traced repo itself.\n",
    "        return str(thm.theorem.file_path)\n",
    "    else:\n",
    "        # The theorem belongs to one of the dependencies.\n",
    "        return f\"{LEAN3_DEPS_DIR}/{thm.repo.name}/{thm.theorem.file_path}\"\n",
    "\n",
    "\n",
    "def export_proofs(\n",
    "    traced_repo: TracedRepo, splits: Dict[SPLIT_STRATEGY, SPLIT], dst_path: Path\n",
    ") -> None:\n",
    "    \"\"\"Export all proofs in a traced repo to ``dst_path''.\"\"\"\n",
    "    for strategy, split in splits.items():\n",
    "        split_dir = dst_path / strategy\n",
    "        split_dir.mkdir(parents=True)\n",
    "\n",
    "        for name, theorems in split.items():\n",
    "            data = []\n",
    "            num_tactics = 0\n",
    "\n",
    "            for thm in theorems:\n",
    "                tactics = [\n",
    "                    {\n",
    "                        \"tactic\": t.tactic,\n",
    "                        \"annotated_tactic\": t.get_annotated_tactic(),\n",
    "                        \"state_before\": t.state_before,\n",
    "                        \"state_after\": t.state_after,\n",
    "                    }\n",
    "                    for t in thm.get_traced_tactics()\n",
    "                    if t.state_before != \"no goals\"\n",
    "                    and \"·\" not in t.tactic  # Ignore \"·\".\n",
    "                ]\n",
    "                num_tactics += len(tactics)\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"url\": thm.repo.url,\n",
    "                        \"commit\": thm.repo.commit,\n",
    "                        \"file_path\": _get_file_path(traced_repo, thm),\n",
    "                        \"full_name\": thm.theorem.full_name,\n",
    "                        \"start\": list(thm.start),\n",
    "                        \"end\": list(thm.end),\n",
    "                        \"traced_tactics\": tactics,\n",
    "                    }\n",
    "                )\n",
    "            oup_path = split_dir / f\"{name}.json\"\n",
    "            json.dump(data, oup_path.open(\"wt\"))\n",
    "            logger.info(\n",
    "                f\"{len(theorems)} theorems and {num_tactics} tactics saved to {oup_path}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def export_premises(traced_repo: TracedRepo, dst_path: Path) -> None:\n",
    "    \"\"\"Export all premise definitions in a traced repo to ``dst_path``.\"\"\"\n",
    "    oup_path = dst_path / \"corpus.jsonl\"\n",
    "    num_premises = 0\n",
    "\n",
    "    with oup_path.open(\"wt\") as oup:\n",
    "        G = traced_repo.traced_files_graph\n",
    "\n",
    "        for tf_node in reversed(list(nx.topological_sort(G))):\n",
    "            tf = G.nodes[tf_node][\"traced_file\"]\n",
    "            imports = [str(_) for _ in G.successors(tf_node)]\n",
    "            premises = tf.get_premise_definitions()\n",
    "            num_premises += len(premises)\n",
    "            oup.write(\n",
    "                json.dumps(\n",
    "                    {\"path\": str(tf.path), \"imports\": imports, \"premises\": premises}\n",
    "                )\n",
    "                + \"\\n\"\n",
    "            )\n",
    "    logger.info(\n",
    "        f\"{num_premises} theorems/definitions from {traced_repo.num_traced_files} files saved to {oup_path}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def export_licenses(traced_repo: TracedRepo, dst_path: Path) -> None:\n",
    "    \"\"\"Export the licenses of a traced repo and all its dependencies to ``dst_path``.\"\"\"\n",
    "    license_dir = dst_path / \"licenses\"\n",
    "    license_dir.mkdir()\n",
    "    all_repos = [traced_repo.repo] + list(traced_repo.dependencies.values())\n",
    "\n",
    "    for repo in all_repos:\n",
    "        lic = repo.get_license()\n",
    "        if lic is None:\n",
    "            continue\n",
    "        with (license_dir / repo.name).open(\"wt\") as oup:\n",
    "            oup.write(lic)\n",
    "\n",
    "    with (license_dir / \"README.md\").open(\"wt\") as oup:\n",
    "        oup.write(\n",
    "            \"This directory contains licenses of Lean repos used to generate this dataset. The dataset itself is released under [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/).\"\n",
    "        )\n",
    "\n",
    "\n",
    "def export_metadata(traced_repo: TracedRepo, dst_path: Path, **kwargs) -> None:\n",
    "    \"\"\"Export the metadata of a traced repo to ``dst_path''.\"\"\"\n",
    "    metadata = dict(kwargs)\n",
    "    metadata[\"creation_time\"] = str(datetime.now())\n",
    "    metadata[\"from_repo\"] = {\n",
    "        \"url\": traced_repo.repo.url,\n",
    "        \"commit\": traced_repo.repo.commit,\n",
    "    }\n",
    "    metadata[\"leandojo_version\"] = lean_dojo.__version__\n",
    "    json.dump(metadata, (dst_path / \"metadata.json\").open(\"wt\"))\n",
    "\n",
    "\n",
    "def export_data(\n",
    "    traced_repo: TracedRepo,\n",
    "    splits: Dict[SPLIT_STRATEGY, SPLIT],\n",
    "    dst_path: Union[str, Path],\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    \"\"\"Export a traced repo whose theorems have been splitted to ``dst_path``.\"\"\"\n",
    "    if isinstance(dst_path, str):\n",
    "        dst_path = Path(dst_path)\n",
    "    if dst_path.exists():\n",
    "        logger.warning(f\"{dst_path} already exists. Removing it now.\")\n",
    "        shutil.rmtree(dst_path)\n",
    "\n",
    "    # Export the proofs.\n",
    "    export_proofs(traced_repo, splits, dst_path)\n",
    "\n",
    "    # Export the premises (theorems, definitions, etc.).\n",
    "    export_premises(traced_repo, dst_path)\n",
    "\n",
    "    # Export the licenses.\n",
    "    export_licenses(traced_repo, dst_path)\n",
    "\n",
    "    # Export metadata.\n",
    "    export_metadata(traced_repo, dst_path, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f5d03",
   "metadata": {},
   "source": [
    "Putting everything together, we're ready to generate the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59eaebb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-12 11:35:36.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlean_dojo.data_extraction.trace\u001b[0m:\u001b[36mtrace\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mLoading the traced repo from /home/kaiyu/.cache/lean_dojo/leanprover-community-mathlib-19c869efa56bbb8b500f2724c0b77261edbfa28c/mathlib\u001b[0m\n",
      "2023-10-12 11:35:38,564\tINFO worker.py:1633 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "100%|███████████████████████████████████████| 3384/3384 [05:26<00:00, 10.38it/s]\n",
      "\u001b[32m2023-10-12 11:41:37.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_data\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1m98734 theorems in total\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:37.611\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_randomly\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mSplitting the theorems randomly\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:37.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_by_premise\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mSplitting the theorems by premises\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:47.299\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:47.300\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:47.373\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.ind\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:48.384\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.ind\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:48.905\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.147\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.ind\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.217\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.218\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.219\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.220\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.221\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.221\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.222\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.223\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.224\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.225\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.225\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.lift\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.225\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.lift\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.226\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.227\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.227\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.228\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.228\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.230\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:52.903\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.ind\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:53.603\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:53.610\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.520\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.521\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.522\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.523\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.524\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.524\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.525\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.526\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.527\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.527\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.528\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.lift\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.528\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.lift\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.530\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.531\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-12 11:41:54.531\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:54.532\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:55.717\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:55.717\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:59.216\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:41:59.217\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:08.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_proofs\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1m94734 theorems and 208539 tactics saved to ../leandojo_benchmark/random/train.json\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:09.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_proofs\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1m2000 theorems and 4721 tactics saved to ../leandojo_benchmark/random/val.json\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:09.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_proofs\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1m2000 theorems and 4516 tactics saved to ../leandojo_benchmark/random/test.json\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:16.737\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:16.738\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:16.813\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:16.814\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:17.275\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.019\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.020\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.025\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.026\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.027\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.027\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.028\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.029\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.029\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.030\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.030\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.lift\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.033\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.lift\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.037\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.044\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.045\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.046\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.047\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.047\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.067\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.ind\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.069\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.ind\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.342\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.ind\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.343\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.ind\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.344\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.345\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.345\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.345\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.346\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.346\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.347\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.347\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.347\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.348\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-12 11:42:19.348\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.lift\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.348\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.lift\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.349\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.349\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.350\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.350\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.350\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.351\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.352\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:19.352\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:28.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_proofs\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1m94734 theorems and 191654 tactics saved to ../leandojo_benchmark/novel_premises/train.json\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:30.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_proofs\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1m2000 theorems and 13103 tactics saved to ../leandojo_benchmark/novel_premises/val.json\u001b[0m\n",
      "\u001b[32m2023-10-12 11:42:31.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_proofs\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1m2000 theorems and 13019 tactics saved to ../leandojo_benchmark/novel_premises/test.json\u001b[0m\n",
      "\u001b[32m2023-10-12 11:43:15.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_premises\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1m130283 theorems/definitions from 3384 files saved to ../leandojo_benchmark/corpus.jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "repo = LeanGitRepo(URL, COMMIT)\n",
    "traced_repo = trace(repo)\n",
    "splits = split_data(traced_repo)\n",
    "export_data(traced_repo, splits, DST_DIR, dataset_name=\"LeanDojo Benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f69c59",
   "metadata": {},
   "source": [
    "The warnings above are expected. It's not clear why we have problems locating a few premises related to `quot`, but we'll ignore them for now since they are only a tiny fraction of all premises. Please let us know if you have any ideas!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0267cf",
   "metadata": {},
   "source": [
    "## Data Format\n",
    "\n",
    "This is the resulting data directory:\n",
    "\n",
    "```\n",
    "├─corpus.jsonl\n",
    "├─metadata.json\n",
    "├─licenses\n",
    "│ ├─lean\n",
    "│ ├─mathlib\n",
    "│ └─README.md\n",
    "├─random\n",
    "│ ├─train.json\n",
    "│ ├─val.json\n",
    "│ └─test.json\n",
    "└─novel_premises\n",
    "  ├─train.json\n",
    "  ├─val.json\n",
    "  └─test.json\n",
    "```\n",
    "\n",
    "`corpus.jsonl` is a corpus of all theorems and definitions in mathlib that can potentially be used as premises. Sub-directories `random` and `novel_premise` are different strategies for splitting the theorems. For each strategy, we have `*.json` files for train/val/test. The sub-directory `licenses` contains license information.\n",
    "\n",
    "### Corpus of Potential Premises\n",
    "\n",
    "`corpus.jsonl` is in [JSON Lines format](https://jsonlines.org/); a line includes the potential premises defined in a single `*.lean` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b97c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3384\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../leandojo_benchmark/corpus.jsonl | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8530908",
   "metadata": {},
   "source": [
    "Let's look at one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eefe7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['path', 'imports', 'premises'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_path = DST_DIR / \"corpus.jsonl\"\n",
    "lines = list(corpus_path.open())\n",
    "file_in_corpus = json.loads(lines[1000])\n",
    "file_in_corpus.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165da93",
   "metadata": {},
   "source": [
    "We can check the file's path and other files it imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e1740f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('src/algebra/gcd_monoid/finset.lean',\n",
       " ['src/data/finset/fold.lean',\n",
       "  '_target/deps/lean/library/init/default.lean',\n",
       "  'src/algebra/gcd_monoid/multiset.lean'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_in_corpus[\"path\"], file_in_corpus[\"imports\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04e6e71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_in_corpus[\"premises\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675ab3e",
   "metadata": {},
   "source": [
    "We can inspect the first potential premise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4b447b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_name': 'finset.lcm',\n",
       " 'code': 'def lcm (s : finset β) (f : β → α) : α := s.fold gcd_monoid.lcm 1 f',\n",
       " 'start': [43, 1],\n",
       " 'end': [43, 68],\n",
       " 'kind': 'definition'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_in_corpus[\"premises\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9d171",
   "metadata": {},
   "source": [
    "Each premise has a fully qualified name, its definition (in the form of Lean code), and the exact location it is defined.\n",
    "\n",
    "\n",
    "### Theorems/Proofs Data\n",
    "\n",
    "Now let's take a look at the theorems/proofs data, taking the `random` split as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a59b38b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94734"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = DST_DIR / \"random/train.json\"\n",
    "proofs_train = json.load(train_path.open())\n",
    "len(proofs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601c5b6",
   "metadata": {},
   "source": [
    "Each element in `proofs_val` represents a theorem. Let's check one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dc67dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['url', 'commit', 'file_path', 'full_name', 'start', 'end', 'traced_tactics'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for proof in proofs_train[::-1]:\n",
    "    if proof[\"traced_tactics\"] != []:\n",
    "        break\n",
    "proof.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc2bc340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://github.com/leanprover-community/mathlib',\n",
       " '19c869efa56bbb8b500f2724c0b77261edbfa28c',\n",
       " 'src/category_theory/limits/has_limits.lean',\n",
       " \"category_theory.limits.colimit.pre_map'\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proof[\"url\"], proof[\"commit\"], proof[\"file_path\"], proof[\"full_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04538c5",
   "metadata": {},
   "source": [
    "We see the theorem's name and where it is defined. The theorem includes some traced tactics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00315e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(proof[\"traced_tactics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc7db5e",
   "metadata": {},
   "source": [
    "Let's look at a traced tactic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52769d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tactic': 'simp [← category.assoc]',\n",
       " 'annotated_tactic': ['simp [← <a>category.assoc</a>]',\n",
       "  [{'full_name': 'category_theory.category.assoc',\n",
       "    'def_path': 'src/category_theory/category/basic.lean',\n",
       "    'def_pos': [109, 1]}]],\n",
       " 'state_before': 'J : Type u₁,\\n_inst_1 : category J,\\nK : Type u₂,\\n_inst_2 : category K,\\nC : Type u,\\n_inst_3 : category C,\\n_inst_4 : has_colimits_of_shape J C,\\n_inst_5 : has_colimits_of_shape K C,\\nF : J ⥤ C,\\nE₁ E₂ : K ⥤ J,\\nα : E₁ ⟶ E₂,\\nj : K\\n⊢ colimit.ι (E₁ ⋙ F) j ≫ colimit.pre F E₁ =\\n    colimit.ι (E₁ ⋙ F) j ≫ colim.map (whisker_right α F) ≫ colimit.pre F E₂',\n",
       " 'state_after': 'no goals'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proof[\"traced_tactics\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df3548",
   "metadata": {},
   "source": [
    "`annotated_tactic` is the tactic with premises annotated by `<a> ... </a>`. For each premise, we know its fully qualified name and the exact location it is defined, which is invaluable for training machine learning models for premise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2782d789",
   "metadata": {},
   "source": [
    "## MiniF2F and ProofNet\n",
    "\n",
    "Similarly, we extract datasets from [miniF2F](https://github.com/facebookresearch/miniF2F) and [ProofNet](https://github.com/zhangir-azerbayev/ProofNet), which are used for evaluation in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc457ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-12 11:46:32.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlean_dojo.data_extraction.trace\u001b[0m:\u001b[36mtrace\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mLoading the traced repo from /home/kaiyu/.cache/lean_dojo/facebookresearch-miniF2F-5271ddec788677c815cf818a06f368ef6498a106/miniF2F\u001b[0m\n",
      "2023-10-12 11:46:37,440\tINFO worker.py:1633 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "100%|███████████████████████████████████████| 1159/1159 [02:48<00:00,  6.86it/s]\n",
      "\u001b[32m2023-10-12 11:49:46.448\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_data\u001b[0m:\u001b[36m118\u001b[0m - \u001b[33m\u001b[1m../leandojo_minif2f already exists. Removing it now.\u001b[0m\n",
      "\u001b[32m2023-10-12 11:49:46.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_proofs\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1m244 theorems and 549 tactics saved to ../leandojo_minif2f/default/val.json\u001b[0m\n",
      "\u001b[32m2023-10-12 11:49:46.511\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlean_dojo.data_extraction.traced_data\u001b[0m:\u001b[36m_callback3\u001b[0m:\u001b[36m230\u001b[0m - \u001b[33m\u001b[1mUnable to locate quot.mk\u001b[0m\n",
      "\u001b[32m2023-10-12 11:49:46.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_proofs\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1m245 theorems and 781 tactics saved to ../leandojo_minif2f/default/test.json\u001b[0m\n",
      "\u001b[32m2023-10-12 11:50:04.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_premises\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1m67170 theorems/definitions from 1159 files saved to ../leandojo_minif2f/corpus.jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "minif2f = LeanGitRepo(\n",
    "    \"https://github.com/facebookresearch/miniF2F\",\n",
    "    \"5271ddec788677c815cf818a06f368ef6498a106\",\n",
    ")\n",
    "traced_minif2f = trace(minif2f)\n",
    "\n",
    "splits = {\"default\": {\"val\": [], \"test\": []}}\n",
    "\n",
    "for tf in traced_minif2f.get_traced_theorems():\n",
    "    if tf.repo.name != \"miniF2F\":\n",
    "        continue\n",
    "    if tf.file_path.name == \"valid.lean\":\n",
    "        splits[\"default\"][\"val\"].append(tf)\n",
    "    else:\n",
    "        assert tf.file_path.name == \"test.lean\"\n",
    "        splits[\"default\"][\"test\"].append(tf)\n",
    "\n",
    "export_data(\n",
    "    traced_minif2f, splits, \"../leandojo_minif2f\", dataset_name=\"LeanDojo MiniF2F\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12f436c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-12 11:50:06.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlean_dojo.data_extraction.trace\u001b[0m:\u001b[36mtrace\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mLoading the traced repo from /home/kaiyu/.cache/lean_dojo/zhangir-azerbayev-ProofNet-e8645aa830ce17c33a8b8482a8195f0f97d6a74a/ProofNet\u001b[0m\n",
      "2023-10-12 11:50:11,771\tINFO worker.py:1633 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "100%|███████████████████████████████████████| 1539/1539 [02:14<00:00, 11.44it/s]\n",
      "\u001b[32m2023-10-12 11:52:48.557\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_data\u001b[0m:\u001b[36m118\u001b[0m - \u001b[33m\u001b[1m../leandojo_proofnet already exists. Removing it now.\u001b[0m\n",
      "\u001b[32m2023-10-12 11:52:48.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_proofs\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1m374 theorems and 460 tactics saved to ../leandojo_proofnet/default/test.json\u001b[0m\n",
      "\u001b[32m2023-10-12 11:53:11.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_premises\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1m82365 theorems/definitions from 1539 files saved to ../leandojo_proofnet/corpus.jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "proofnet = LeanGitRepo(\n",
    "    \"https://github.com/zhangir-azerbayev/ProofNet\",\n",
    "    \"e8645aa830ce17c33a8b8482a8195f0f97d6a74a\",\n",
    ")\n",
    "traced_proofnet = trace(proofnet)\n",
    "splits = {\n",
    "    \"default\": {\n",
    "        \"test\": [\n",
    "            tf\n",
    "            for tf in traced_proofnet.get_traced_theorems()\n",
    "            if tf.repo.name == \"ProofNet\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "export_data(\n",
    "    traced_proofnet, splits, \"../leandojo_proofnet\", dataset_name=\"LeanDojo ProofNet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9f3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
